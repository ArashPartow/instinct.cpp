syntax = "proto3";

package instinct.llm;


message Message {
  string role = 1;
  string content = 2;

  /**
  An name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  string name = 3;

  /**
  The tool calls generated by the model, such as function calls.
   */
  repeated ToolCallObject tool_calls = 4;

  /**
  Tool call that this message is responding to.
   */
  string tool_call_id = 5;
}

message ToolCallObject {
  string id = 1;
  ToolCallObjectType type = 2;
  message ToolCallFunctionObject {
    string name = 1;
    string arguments = 2;
  }
  ToolCallFunctionObject function = 3;
}
enum ToolCallObjectType {
  unknown_run_tool_call_object_type = 0;
  function = 1;
}

message FunctionTool {
  /**
  A description of what the function does, used by the model to choose when and how to call the function.
   */
  string description = 1;

  /**
  The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   */
  string name = 2;

  /**
  The parameters the functions accepts, described as a JSON Schema object. Omitting parameters defines a function with an empty parameter list.
   */
  FunctionParametersSchema parameters = 3;
  message FunctionParametersSchema {
    string type = 1; // always 'object'
    map<string, FunctionParameterSchema> properties = 2;
    message FunctionParameterSchema {
      string type = 1;
      string description = 2;
    }
  }
  repeated string required = 4;
}

message OpenAIChatCompletionRequest {
  /**
  A list of messages comprising the conversation so far.
   */
  repeated Message messages = 1;

  /**
  ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API
   */
  string model = 2;

  /**
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   */
  float frequency_penalty = 3;

  /**
  Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model.
   */
  bool logprobs = 4;

  /**
  An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
   */
  bool top_logprobs = 5;


  /**
  The maximum number of tokens that can be generated in the chat completion.
   */
  int32 max_tokens = 6;

  /**
  How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
   */
  int32 n = 7;

  /**
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   */
  float presence_penalty = 8;

  /**
  An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.

  Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.
   */
  OpenAIResponseFormat response_format = 9;
  message OpenAIResponseFormat {
    string type = 1;
  }

  /**
  This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.
   */
  int32 seed = 10;

  /**
  Up to 4 sequences where the API will stop generating further tokens.
   */
  repeated string stop = 11;

  /**
  If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
   */
  optional bool stream = 12;

  /**
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
   */
  float temperature = 13;

  /**
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
   */
  float top_p = 14;

  /**
  A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
   */
  repeated ChatCompletionTool tools = 15;
  message ChatCompletionTool {
    string type = 1; // always function
    FunctionTool function = 2;
  }



  /**
  Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {"type": "function", "function": {"name": "my_function"}} forces the model to call that function.
   */
  string tool_choice = 16;
  //  oneof tool_choice {
  //    /**
  //    none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function.
  //     */
  //    string mode = 16;
  //
  //    /**
  //    Specifies a tool the model should use. Use to force the model to call a specific function.
  //     */
  //    OpenAITool forced_tool = 17;
  //  }

  message OpenAITool {
    /**
    The type of the tool. Currently, only function is supported.
     */
    string type = 1;

    FunctionToolObject object = 2;
    message FunctionToolObject {
      /**
      The name of the function to call.
       */
      string name = 1;
    }
  }

  /**
  A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.
   */
  string user = 18;
}


/**
https://platform.openai.com/docs/api-reference/chat/object
 */
message OpenAIChatCompletionResponse {
  /**
  A unique identifier for the chat completion.
   */
  string id = 1;

  /**
  A list of chat completion choices. Can be more than one if n is greater than 1.
   */
  repeated OpenAIChatCompletionChoice choices = 2;
  message OpenAIChatCompletionChoice {
    /**
    The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.
     */
    string finish_reason = 1;

    /**
    The index of the choice in the list of choices.
     */
    int32 index = 2;

    /**
    A chat completion message generated by the model.
     */
    Message message = 3;

    /**
    Log probability information for the choice.
     */
    OpenAIChatCompletionLogProbs logprobs = 4;

  }

  /**
  The Unix timestamp (in seconds) of when the chat completion was created.
   */
  int64 created = 5;

  /**
  The model used for the chat completion.
   */
  string model = 6;

  /**
  This fingerprint represents the backend configuration that the model runs with.

  Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
   */
  string system_fingerprint = 7;

  /**
  The object type, which is always chat.completion.
   */
  string object = 8;

  /**
  Usage statistics for the completion request.
   */
  OpenAIUsageInResponse usage = 9;

}

message OpenAIUsageInResponse {
  /**
  Number of tokens in the generated completion.
   */
  optional int32 completion_tokens = 1;

  /**
  Number of tokens in the prompt.
   */
  optional int32 prompt_tokens = 2;

  /**
  Total number of tokens used in the request (prompt + completion).
   */
  optional int32 total_tokens = 3;
}

message OpenAIChatCompletionLogProbs {
  /**
  A list of message content tokens with log probability information.
   */
  repeated OpenAIChatCompletionLogProbItem content = 1;
  message OpenAIChatCompletionLogProbItem {
    /**
    The token
     */
    string token = 1;

    /**
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
     */
    float logprob = 2;

    /**
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.
     */
    repeated int32 bytes = 3;

    /**
    List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned.
     */
    repeated OpenAIChatCompletionLogProbItem top_logprobs = 4;
  }
}


message OpenAIChatCompletionChunk {
  string id = 1;

  repeated OpenAIChatCompletionChunkChoice choices = 2;
  message OpenAIChatCompletionChunkChoice {
    Message delta = 1;
    OpenAIChatCompletionLogProbs logprobs = 2;
    string finish_reason = 3;
    optional int32 index = 4;
  }

  int64 created = 5;

  string model = 6;

  string system_fingerprint = 7;

  string object = 8;
}

