syntax = "proto3";
//import "google/protobuf/any.proto";
import "core.proto";

package instinct.llm;


message LLMChainContext {
  // make it a K-V structure so we can easily merge
  map<string, instinct.core.PrimitiveVariable> values = 1;
}

message Message {
  string role = 1;
  string content = 2;
//  oneof value {
//    AIMessage ai = 1;
//    HumanMessage human = 2;
//    FunctionMessage function = 3;
//    SystemMessage system = 4;
//    ChatMessage chat = 5;
//  }
}

//message AIMessage {
//  string content = 1;
//  bool example = 2;
//}
//
//message HumanMessage {
//  string content = 1;
//  bool example = 2;
//}
//
//message FunctionMessage {
//  string name = 1;
//  string content = 2;
//}
//
//message SystemMessage {
//  string content = 1;
//}
//
//message ChatMessage {
//  string role = 1;
//  string content = 2;
//}

message PromptValue {
  oneof value {
    StringPromptValue string = 1;
    ChatPromptValue chat = 2;
  }
}

message StringPromptValue {
  string text = 1;
}

message ChatPromptValue {
  repeated Message messages = 1;
}

message MessageList {
  repeated Message messages = 1;
}

message PromptExample {
  map<string, instinct.core.PrimitiveVariable> values = 1;
}

message PromptExamples {
  repeated PromptExample values = 1;
}



message Generation {
  string text = 1;
  optional Message message = 2;
  bool is_chunk = 3;
}


//message LanguageModelInput {
//  oneof value {
//    string text = 1;
//    StringPromptValue string_prompt = 2;
//    ChatPromptValue chat_prompt = 3;
//  }
//}

//message BatchedLanguageModelInput {
//  repeated LanguageModelInput inputs = 1;
//}

/**
for both LLM and ChatModel
 */
message LangaugeModelResult {
  repeated Generation generations = 1;
  oneof raw_response {
    OllamaCompletionResponse ollama_completion = 2;
    OllamaChatCompletionResponse ollama_chat = 3;
  }
}

message BatchedLangaugeModelResult {
  //  LangaugeModelOutput won't contain valid `raw_response` in BatchedLangaugeModelOutput, read content of `raw_response` below
  repeated LangaugeModelResult generations = 1;
  oneof raw_response {
    OllamaCompletionResponse ollama_completion = 2;
    OllamaChatCompletionResponse ollama_chat = 3;
  }
}

message OllamaGenerateMessage {
  string role = 1;
  string content = 2;
  repeated string images = 3;
}

message OllamaModelOptions {
  optional int32 temperature = 1;
  int32 seed = 2;
}

message OllamaCompletionRequest {
  string model = 1;
  string prompt = 2;
  optional bool stream = 3;
  string format = 4;
  optional OllamaModelOptions options = 5;
  optional bool raw = 6;
}

message OllamaCompletionResponse {
  string model = 1;
  string created_at = 2;
  repeated int32 context = 3;
  string response = 4;
  optional bool done = 6;
  optional uint64 total_duration = 7;
  optional uint64 load_duration = 8;
  optional int32 prompt_eval_count = 9;
  optional uint32 prompt_eval_duration = 10;
  optional int32 eval_count = 11;
  optional uint64 eval_duration = 12;
}

message OllamaChatCompletionRequest {
  string model = 1;
  repeated OllamaGenerateMessage messages = 2;
  // mark bool as optional or it's ignored if value is false
  optional bool stream = 3;
  string format = 4;
  OllamaModelOptions options = 5;
}

message OllamaChatCompletionResponse {
  string model = 1;
  string created_at = 2;
  repeated int32 context = 3;
  OllamaGenerateMessage message = 5;
  bool done = 6;
  uint64 total_duration = 7;
  uint64 load_duration = 8;
  int32 prompt_eval_count = 9;
  uint32 prompt_eval_duration = 10;
  int32 eval_count = 11;
  uint64 eval_duration = 12;
}

message OllamaEmbeddingRequest {
  string model = 1;
  string prompt = 2;
  optional OllamaModelOptions options = 3;
}

message OllamaEmbeddingResponse {
  repeated float embedding = 1;
}



//message OllamaConfiguration {
//  string model_name = 1;
//  string endpoint_host = 2;
//  int32 endpoint_port = 3;
//  OllamaModelOptions model_options = 4;
//}

//message PromptExampleSelectorConfiguration {
//  oneof value {
//    PromptExamples passthrough = 1;
//    LengthBasedExampleSelectorConfiguration length_based = 2;
//  }
//}
//
//message LengthBasedExampleSelectorConfiguration {
//  int32 max_length = 1;
//}
//
//message FewShotPromptTemplateConfiguration {
//  string prefix = 1;
//  string suffix = 2;
//  string example_seperator = 3;
//  PromptExampleSelectorConfiguration example_selector = 4;
//}
//
//
//message PromptTemplate {
//  oneof value {
//    string text = 1;
//    MessageList chat = 2;
//    FewShotPromptTemplateConfiguration few_shot = 3;
//  }
//}
//
//message LLMConfiguration {
//  oneof value {
//    OllamaConfiguration ollama_chat = 1;
//    OllamaConfiguration ollama_llm = 2;
//  }
//}

//
//message OutputParserConfiguration {
//  oneof value {
//
//  }
//}
//
//
//message LLMChainConfiguration {
//  PromptTemplate prompt = 1;
//  LLMConfiguration llm = 2;
//}
