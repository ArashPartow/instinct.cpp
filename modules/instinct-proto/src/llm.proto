syntax = "proto3";
//import "google/protobuf/any.proto";
import "core.proto";

package instinct.llm;


message LLMChainContext {
  // make it a K-V structure so we can easily merge
  map<string, instinct.core.PrimitiveVariable> values = 1;
}



message Message {
  string role = 1;
  string content = 2;

  /**
  An name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  string name = 3;

  /**
  The tool calls generated by the model, such as function calls.
   */
  repeated ToolRequest tool_calls = 4;
  message ToolRequest {
    /**
    The name of the function to call.
     */
    string name = 1;

    /**
    The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
     */
    string arguments = 2;
  }

  /**
  Tool call that this message is responding to.
   */
  string tool_call_id = 5;
}

message FunctionTool {
  /**
  A description of what the function does, used by the model to choose when and how to call the function.
   */
  string description = 1;

  /**
  The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   */
  string name = 2;

  /**
  The parameters the functions accepts, described as a JSON Schema object. Omitting parameters defines a function with an empty parameter list.
   */
  string parameters = 3;
}

message PromptValue {
  oneof value {
    StringPromptValue string = 1;
    ChatPromptValue chat = 2;
  }
}

message StringPromptValue {
  string text = 1;
}

message ChatPromptValue {
  repeated Message messages = 1;
}

message MessageList {
  repeated Message messages = 1;
}


message Generation {
  string text = 1;
  Message message = 2;
  bool is_chunk = 3;
}


/**
for both LLM and ChatModel
 */
message LangaugeModelResult {
  repeated Generation generations = 1;
  oneof raw_response {
    OllamaCompletionResponse ollama_completion = 2;
    OllamaChatCompletionResponse ollama_chat = 3;
  }
}

message BatchedLangaugeModelResult {
  //  LangaugeModelOutput won't contain valid `raw_response` in BatchedLangaugeModelOutput, read content of `raw_response` below
  repeated LangaugeModelResult generations = 1;
  oneof raw_response {
    OllamaCompletionResponse ollama_completion = 2;
    OllamaChatCompletionResponse ollama_chat = 3;
  }
}


message MultilineGeneration {
  repeated string lines = 1;
}

message StructuredGeneration {

  repeated StructuredBlock blocks = 1;
  Generation raw_generation = 2;

  message StructuredBlock {
    oneof value {
      TextBlock text = 1;
      TabularBlock table = 2;
      TabularBlock image = 3;
    }
  }

  message TextBlock {

  }

  message TabularBlock {

  }

  message ImageBlock {

  }

}


message OllamaGenerateMessage {
  string role = 1;
  string content = 2;
  repeated string images = 3;
}

message OllamaModelOptions {
  int32 temperature = 1;
  int32 seed = 2;
}

message OllamaCompletionRequest {
  string model = 1;
  string prompt = 2;
  optional bool stream = 3;
  string format = 4;
  OllamaModelOptions options = 5;
  bool raw = 6;
}

message OllamaCompletionResponse {
  string model = 1;
  string created_at = 2;
  repeated int32 context = 3;
  string response = 4;
  bool done = 6;
  uint64 total_duration = 7;
  uint64 load_duration = 8;
  int32 prompt_eval_count = 9;
  uint32 prompt_eval_duration = 10;
  int32 eval_count = 11;
  uint64 eval_duration = 12;
}

message OllamaChatCompletionRequest {
  string model = 1;
  repeated OllamaGenerateMessage messages = 2;
  // mark bool as or it's ignored if value is false
  optional bool stream = 3;
  string format = 4;
  OllamaModelOptions options = 5;
}

message OllamaChatCompletionResponse {
  string model = 1;
  string created_at = 2;
  repeated int32 context = 3;
  OllamaGenerateMessage message = 5;
  bool done = 6;
  uint64 total_duration = 7;
  uint64 load_duration = 8;
  int32 prompt_eval_count = 9;
  uint32 prompt_eval_duration = 10;
  int32 eval_count = 11;
  uint64 eval_duration = 12;
}

message OllamaEmbeddingRequest {
  string model = 1;
  string prompt = 2;
  OllamaModelOptions options = 3;
}

message OllamaEmbeddingResponse {
  repeated float embedding = 1;
}




message OpenAIChatCompletionRequest {
  /**
  A list of messages comprising the conversation so far.
   */
  repeated Message messages = 1;

  /**
  ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API
   */
  string model = 2;

  /**
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   */
  float frequency_penalty = 3;

  /**
  Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model.
   */
  bool logprobs = 4;

  /**
  An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
   */
  bool top_logprobs = 5;


  /**
  The maximum number of tokens that can be generated in the chat completion.
   */
  int32 max_token = 6;

  /**
  How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
   */
  int32 n = 7;

  /**
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   */
  float presence_penalty = 8;

  /**
  An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.

  Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.
   */
  OpenAIResponseFormat response_format = 9;
  message OpenAIResponseFormat {
    string type = 1;
  }

  /**
  This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.
   */
  int32 seed = 10;

  /**
  Up to 4 sequences where the API will stop generating further tokens.
   */
  repeated string stop = 11;

  /**
  If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
   */
  optional bool stream = 12;

  /**
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
   */
  float temperature = 13;

  /**
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
   */
  float top_p = 14;

  /**
  A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
   */
  repeated FunctionTool tools = 15;



  /**
  Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {"type": "function", "function": {"name": "my_function"}} forces the model to call that function.
   */
  string tool_choice = 16;
//  oneof tool_choice {
//    /**
//    none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function.
//     */
//    string mode = 16;
//
//    /**
//    Specifies a tool the model should use. Use to force the model to call a specific function.
//     */
//    OpenAITool forced_tool = 17;
//  }

  message OpenAITool {
    /**
    The type of the tool. Currently, only function is supported.
     */
    string type = 1;

    FunctionToolObject object = 2;
    message FunctionToolObject {
      /**
      The name of the function to call.
       */
      string name = 1;
    }
  }

  /**
  A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.
   */
  string user = 18;
}


message OpenAIChatCompletionResponse {
  /**
  A unique identifier for the chat completion.
   */
  string id = 1;

  /**
  A list of chat completion choices. Can be more than one if n is greater than 1.
   */
  repeated OpenAIChatCompletionChoice choices = 2;
  message OpenAIChatCompletionChoice {
    /**
    The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.
     */
    string finish_reason = 1;

    /**
    The index of the choice in the list of choices.
     */
    int32 index = 2;

    /**
    A chat completion message generated by the model.
     */
    Message message = 3;

    /**
    Log probability information for the choice.
     */
    repeated OpenAIChatCompletionLogProbs logprobs = 4;

  }

  /**
  The Unix timestamp (in seconds) of when the chat completion was created.
   */
  int64 created = 5;

  /**
  The model used for the chat completion.
   */
  string model = 6;

  /**
  This fingerprint represents the backend configuration that the model runs with.

  Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
   */
  string system_fingerprint = 7;

  /**
  The object type, which is always chat.completion.
   */
  string object = 8;

  /**
  Usage statistics for the completion request.
   */
  OpenAIUsageInResponse usage = 9;

}

message OpenAIUsageInResponse {
  /**
  Number of tokens in the generated completion.
   */
  int32 completion_tokens = 1;

  /**
  Number of tokens in the prompt.
   */
  int32 prompt_tokens = 2;

  /**
  Total number of tokens used in the request (prompt + completion).
   */
  int32 total_tokens = 3;
}

message OpenAIChatCompletionLogProbs {
  /**
  A list of message content tokens with log probability information.
   */
  OpenAIChatCompletionLogProbItem content = 1;
  message OpenAIChatCompletionLogProbItem {
    /**
    The token
     */
    string token = 1;

    /**
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
     */
    float logprob = 2;

    /**
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.
     */
    repeated int32 bytes = 3;

    /**
    List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned.
     */
    repeated OpenAIChatCompletionLogProbItem top_logprobs = 4;
  }
}


message OpenAIChatCompletionChunk {
  string id = 1;

  repeated OpenAIChatCompletionChunkChoice choices = 2;
  message OpenAIChatCompletionChunkChoice {
    Message delta = 1;
    OpenAIChatCompletionLogProbs logprobs = 2;
    string finish_reason = 3;
    int32 index = 4;
  }

  int64 created = 5;

  string model = 6;

  string system_fingerprint = 7;

  string object = 8;
}

message OpenAIEmbeddingRequest {
  /**
  Input text to embed
   */
  repeated string input = 1;

  /**
  ID of the model to use. You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.
   */
  string model = 2;

  /**
  The format to return the embeddings in. Can be either float or base64.
   */
  string encoding_format = 3;

  /**
  The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.
   */
  uint32 dimension = 4;

  /**
  A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
   */
  string user = 5;
}

message OpenAIEmbeddingResponse {
  string object = 1;

  repeated OpenAIEmbedding data = 2;
  message OpenAIEmbedding {
    int32 index = 1;
    repeated float embedding = 2;
    string object = 3;
  }

  string model = 3;

  OpenAIUsageInResponse usage = 4;
}
