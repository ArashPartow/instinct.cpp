{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:02:07.675332Z",
     "start_time": "2024-03-28T12:02:07.672048Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/cookbook/en/rag_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:03:50.247102Z",
     "start_time": "2024-03-28T12:02:07.676472Z"
    }
   },
   "id": "3c7ba6dbea18e2ef",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:03:50.266390Z",
     "start_time": "2024-03-28T12:03:50.249378Z"
    }
   },
   "id": "cd3959f3217bfa17",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:03:53.134585Z",
     "start_time": "2024-03-28T12:03:53.119922Z"
    }
   },
   "id": "cdd427a5684551d2",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41e74e8131e247108b2be231f4e19fb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 22.0M/22.0M [00:02<00:00, 10.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e12585164214541a2bfa37c9e984408"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:10:51.681718Z",
     "start_time": "2024-03-28T12:10:40.997869Z"
    }
   },
   "id": "194526c689413e8d",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2647 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bd64c40da13434fabba70fb48c698dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:16:24.092647Z",
     "start_time": "2024-03-28T12:16:23.417905Z"
    }
   },
   "id": "43c0568919a65e8c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nGreat! How can I assist you in this test context? Please provide more information or clarify what you need help with.'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = ChatOllama(model = \"llama2:latest\")\n",
    "\n",
    "\n",
    "def call_llm(chat_ollama: ChatOllama, prompt: str):\n",
    "    response = chat_ollama.invoke(input=prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:44:10.460643Z",
     "start_time": "2024-03-28T12:44:05.072832Z"
    }
   },
   "id": "865ba644cc890814",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:44:35.626363Z",
     "start_time": "2024-03-28T12:44:35.608822Z"
    }
   },
   "id": "3a855d46ab06e6ec",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 QA couples...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4eb7531ebf14db687fbe54a48fbfd50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:45:39.028600Z",
     "start_time": "2024-03-28T12:45:17.305702Z"
    }
   },
   "id": "726778ed12d58f57",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          context  \\\n0  Note: Hugging Face's [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipelines) makes it incredibly easy to pull in open source ML models like transformers with just a single line of code.\\n\\n### 7.1 Install Transformers\\n\\nFirst, let's install Transformers via the following code:\\n\\n```python\\n!pip install transformers\\n```\\n\\n### 7.2 Try out BERT\\n\\nFeel free to swap out the sentence below for one of your own. However, leave [MASK] in somewhere to allow BERT to predict the missing word\\n\\n```python\\nfrom transformers import pipeline\\nunmasker = pipeline('fill-mask', model='bert-base-uncased')\\nunmasker(\"Artificial Intelligence [MASK] take over the world.\")\\n```\\n\\nWhen you run the above code you should see an output like this:\\n\\n```\\n[{'score': 0.3182411789894104,\\n  'sequence': 'artificial intelligence can take over the world.',\\n  'token': 2064,\\n  'token_str': 'can'},\\n {'score': 0.18299679458141327,\\n  'sequence': 'artificial intelligence will take over the world.',\\n  'token': 2097,\\n  'token_str': 'will'},\\n {'score': 0.05600147321820259,\\n  'sequence': 'artificial intelligence to take over the world.',\\n  'token': 2000,\\n  'token_str': 'to'},\\n {'score': 0.04519503191113472,\\n  'sequence': 'artificial intelligences take over the world.',\\n  'token': 2015,\\n  'token_str': '##s'},\\n {'score': 0.045153118669986725,\\n  'sequence': 'artificial intelligence would take over the world.',\\n  'token': 2052,\\n  'token_str': 'would'}]\\n```\\n\\nKind of frightening right? 🙃\\n\\n### 7.3 Be aware of model bias\\n\\nLet's see what jobs BERT suggests for a \"man\":\\n\\n```python\\nunmasker(\"The man worked as a [MASK].\")\\n```\\n\\nWhen you run the above code you should see an output that looks something like:   \n\n                                                         question  \\\n0  What is the most common job suggested by BERT for a \"man\"?\\n\\n   \n\n                                                                                     answer  \\\n0  According to the context, the most common job suggested by BERT for a \"man\" is \"worker\".   \n\n                               source_doc  \n0  huggingface/blog/blob/main/bert-101.md  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>source_doc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Note: Hugging Face's [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipelines) makes it incredibly easy to pull in open source ML models like transformers with just a single line of code.\\n\\n### 7.1 Install Transformers\\n\\nFirst, let's install Transformers via the following code:\\n\\n```python\\n!pip install transformers\\n```\\n\\n### 7.2 Try out BERT\\n\\nFeel free to swap out the sentence below for one of your own. However, leave [MASK] in somewhere to allow BERT to predict the missing word\\n\\n```python\\nfrom transformers import pipeline\\nunmasker = pipeline('fill-mask', model='bert-base-uncased')\\nunmasker(\"Artificial Intelligence [MASK] take over the world.\")\\n```\\n\\nWhen you run the above code you should see an output like this:\\n\\n```\\n[{'score': 0.3182411789894104,\\n  'sequence': 'artificial intelligence can take over the world.',\\n  'token': 2064,\\n  'token_str': 'can'},\\n {'score': 0.18299679458141327,\\n  'sequence': 'artificial intelligence will take over the world.',\\n  'token': 2097,\\n  'token_str': 'will'},\\n {'score': 0.05600147321820259,\\n  'sequence': 'artificial intelligence to take over the world.',\\n  'token': 2000,\\n  'token_str': 'to'},\\n {'score': 0.04519503191113472,\\n  'sequence': 'artificial intelligences take over the world.',\\n  'token': 2015,\\n  'token_str': '##s'},\\n {'score': 0.045153118669986725,\\n  'sequence': 'artificial intelligence would take over the world.',\\n  'token': 2052,\\n  'token_str': 'would'}]\\n```\\n\\nKind of frightening right? 🙃\\n\\n### 7.3 Be aware of model bias\\n\\nLet's see what jobs BERT suggests for a \"man\":\\n\\n```python\\nunmasker(\"The man worked as a [MASK].\")\\n```\\n\\nWhen you run the above code you should see an output that looks something like:</td>\n      <td>What is the most common job suggested by BERT for a \"man\"?\\n\\n</td>\n      <td>According to the context, the most common job suggested by BERT for a \"man\" is \"worker\".</td>\n      <td>huggingface/blog/blob/main/bert-101.md</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:49:11.248364Z",
     "start_time": "2024-03-28T12:49:11.227488Z"
    }
   },
   "id": "9a32f8c37af71bc8",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:52:05.711659Z",
     "start_time": "2024-03-28T12:52:05.696196Z"
    }
   },
   "id": "4e60ad11cd96e14c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2d8f15883c8480abf768ce313fd5729"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:53:27.374143Z",
     "start_time": "2024-03-28T12:52:20.188747Z"
    }
   },
   "id": "8ba11a71e2b2e890",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/915 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e121d7098a8c4fafb4050e915fafebe9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 297k/297k [00:01<00:00, 204kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/67 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4382460eb0d41ed872cc47a33579389"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:20.931818Z",
     "start_time": "2024-03-28T12:55:07.781532Z"
    }
   },
   "id": "921990aa1cb355f0",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2647 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb33cb98d77f4ecab97db61d8df907b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:55.725063Z",
     "start_time": "2024-03-28T12:55:55.659416Z"
    }
   },
   "id": "11ea5ea55c0da346",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:56:34.466166Z",
     "start_time": "2024-03-28T12:56:33.566851Z"
    }
   },
   "id": "40aa9cb71de4a46a",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:56:57.804716Z",
     "start_time": "2024-03-28T12:56:57.734204Z"
    }
   },
   "id": "13dc9ec746ddadd2",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:09:56.364198Z",
     "start_time": "2024-03-28T13:09:56.342186Z"
    }
   },
   "id": "851eb803b3f0a34f",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragatouille in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (0.0.8.post2)\r\n",
      "Requirement already satisfied: colbert-ai==0.2.19 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (0.2.19)\r\n",
      "Requirement already satisfied: faiss-cpu<2.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (1.8.0)\r\n",
      "Requirement already satisfied: fast-pytorch-kmeans==0.2.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (0.2.0.1)\r\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (0.1.10)\r\n",
      "Requirement already satisfied: langchain_core<0.2.0,>=0.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (0.1.29)\r\n",
      "Requirement already satisfied: llama-index>=0.7 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (0.10.25)\r\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (1.16.0)\r\n",
      "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (2.6.1)\r\n",
      "Requirement already satisfied: srsly==2.4.8 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (2.4.8)\r\n",
      "Requirement already satisfied: torch>=1.13 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (2.2.2)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (4.39.1)\r\n",
      "Requirement already satisfied: voyager<3.0.0,>=2.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from ragatouille) (2.0.6)\r\n",
      "Requirement already satisfied: bitarray in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (2.9.2)\r\n",
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (2.18.0)\r\n",
      "Requirement already satisfied: flask in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (3.0.2)\r\n",
      "Requirement already satisfied: git-python in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.3)\r\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.1)\r\n",
      "Requirement already satisfied: ninja in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (1.11.1.1)\r\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (1.12.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (4.66.2)\r\n",
      "Requirement already satisfied: ujson in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from colbert-ai==0.2.19->ragatouille) (5.9.0)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\r\n",
      "Requirement already satisfied: pynvml in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (11.5.0)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from srsly==2.4.8->ragatouille) (2.0.10)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.28)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (3.9.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.6.4)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.33)\r\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.25)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.1)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.18)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.6.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\r\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (4.3.0)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (23.2)\r\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.2.1)\r\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.11)\r\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.25 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.10.25.post1)\r\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.7)\r\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.5)\r\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.9.48)\r\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.13)\r\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.4)\r\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.5)\r\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.3)\r\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.12)\r\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index>=0.7->ragatouille) (0.1.4)\r\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (5.26.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.4.1.post1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.22.1)\r\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (10.2.0)\r\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from torch>=1.13->ragatouille) (3.13.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from torch>=1.13->ragatouille) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from torch>=1.13->ragatouille) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from torch>=1.13->ragatouille) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from torch>=1.13->ragatouille) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from torch>=1.13->ragatouille) (2024.2.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.9.4)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (3.4)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.3.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->ragatouille) (2.4)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain<0.2.0,>=0.1.0->ragatouille) (3.9.15)\r\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.2.14)\r\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.0.8)\r\n",
      "Requirement already satisfied: httpx in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (0.27.0)\r\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.15 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (0.1.15)\r\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.6.0)\r\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (3.8.1)\r\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.14.1)\r\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (2.2.1)\r\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (0.6.0)\r\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.12.3)\r\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (0.0.2)\r\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (1.24.0)\r\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.1.0)\r\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (0.0.26)\r\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index>=0.7->ragatouille) (0.4.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.16.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2024.2.2)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (15.0.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\r\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\r\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.1)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.1.2)\r\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (8.1.7)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.7.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from jinja2->torch>=1.13->ragatouille) (2.1.5)\r\n",
      "Requirement already satisfied: gitpython in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.42)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from sympy->torch>=1.13->ragatouille) (1.3.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (2.5)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.16.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.0.4)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (0.14.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.9.0)\r\n",
      "Requirement already satisfied: PyMuPDFb==1.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (1.24.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (3.0.3)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (1.0.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.11)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (2.9.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (2024.1)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.25->llama-index>=0.7->ragatouille) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ragatouille\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:11:35.445867Z",
     "start_time": "2024-03-28T13:11:31.736935Z"
    }
   },
   "id": "3c7dc5ad854a95cb",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:11:56.417566Z",
     "start_time": "2024-03-28T13:11:53.732301Z"
    }
   },
   "id": "283a72f828eb8ec3",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:13:14.583163Z",
     "start_time": "2024-03-28T13:13:14.511514Z"
    }
   },
   "id": "913cbbdae2722e2d",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:13:33.679498Z",
     "start_time": "2024-03-28T13:13:33.619423Z"
    }
   },
   "id": "c193a75198dabe56",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "eval_chat_model = ChatOllama(model=\"mixtral:latest\", temperature=0)\n",
    "evaluator_name = \"mixtral-7bx8\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:14:40.891052Z",
     "start_time": "2024-03-28T13:14:40.830815Z"
    }
   },
   "id": "31856cff24422023",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "READER_MODEL_NAME = \"llama2:latest\"\n",
    "READER_LLM = Ollama(model = READER_MODEL_NAME)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:32:36.342560Z",
     "start_time": "2024-03-29T00:32:36.285420Z"
    }
   },
   "id": "da376c9fbaaa20f",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:llama2:latest:\n",
      "Loading knowledge base embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1146e11e76549b1a3a0c158aff0cc9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57d92c12d1c64a9696ed1c1f08f55d7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2443d18d0cb644db96ffdb73f9c95a7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca586e7064404b21b6fd625d48091bcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db1d9acf9f86474788b774abff97be73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-29T00:32:37.302210Z"
    }
   },
   "id": "83e219af8e9a45c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a38d56476f344fd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
