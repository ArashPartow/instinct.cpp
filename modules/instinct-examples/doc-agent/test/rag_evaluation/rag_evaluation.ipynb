{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install pre-requisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a65285e868d7809"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets ragatouille ratelimit retry"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:23:51.178201Z",
     "start_time": "2024-04-12T00:23:48.608740Z"
    }
   },
   "id": "3c7ba6dbea18e2ef",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:23:51.243526Z",
     "start_time": "2024-04-12T00:23:51.179439Z"
    }
   },
   "id": "cd3959f3217bfa17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model preparations\n",
    "\n",
    "To go through the evaluation process, we need following models:\n",
    "\n",
    "1. Document model: Embedding model to generate document embeddings which will persisted in vector index. \n",
    "2. Reader model: A text completion model to answer the final question with augmented context.\n",
    "3. Evaluator model: A chat completion model that will give final verdict about RAG output. As this model will affect scoring considerably, stronger model should be used. \n",
    "\n",
    "As the choice of different models is not subject of this article and won't impact the comparison between RAG frameworks, we are determined to use completed local solution for this experiment for better speed and lower cost. \n",
    "\n",
    "To be more precise, following models that are already optimized in Ollama are used:\n",
    "\n",
    "* [Gemma 2B](https://huggingface.co/google/gemma-2b) as both `Document model` and `Reader model`.\n",
    "* [Mixtral-8x7B](https://ollama.com/library/mixtral) for `Evaluator model`\n",
    "   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2c4b6af65d58cdf"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama, MiniMaxChat\n",
    "import os\n",
    "\n",
    "\n",
    "READER_MODEL_NAME = \"gemma:2b\"\n",
    "EMBEDDING_NAME = \"all-minilm\"\n",
    "\n",
    "# OLLAMA_BASE_URL = \"http://192.168.0.29:11434\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=EMBEDDING_NAME, base_url = OLLAMA_BASE_URL)\n",
    "READER_LLM = Ollama(model=READER_MODEL_NAME, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "EVALUATOR_NAME = \"mixtral:latest\"\n",
    "EVAL_MODEL = ChatOllama(model=EVALUATOR_NAME, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "\n",
    "# EVALUATOR_NAME = \"abab6-chat\"\n",
    "# EVAL_MODEL = MiniMaxChat(\n",
    "#     model_name=EVALUATOR_NAME, \n",
    "#     minimax_api_key=os.getenv(\"MINIMAX_API_KEY\"),\n",
    "#     minimax_group_id=os.getenv(\"MINIMAX_GROUP_ID\")\n",
    "# )\n",
    "\n",
    "\n",
    "LANGCHAIN_DATA_ROOT = \"./data/langchain\"\n",
    "INSTINCT_DOC_AGENT_DATA_ROOT = \"./data/doc_agent\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:46:00.786680Z",
     "start_time": "2024-04-12T00:46:00.171769Z"
    }
   },
   "id": "b4dfdc109f0daffc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build RAG pipeline using `langchain` \n",
    "\n",
    "1. transform training data in `m-ric/huggingface_doc` to `langchain`'s document objects\n",
    "2. Load into faiss index if index file is absent\n",
    "3. prompt with eval data `m-ric/huggingface_doc` using `READER_MODEL` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bdb8cc674412799"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Knowledge base preparations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881763874245235f"
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:23:51.354401Z",
     "start_time": "2024-04-12T00:23:51.300888Z"
    }
   },
   "id": "cdd427a5684551d2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:23:51.408289Z",
     "start_time": "2024-04-12T00:23:51.355278Z"
    }
   },
   "id": "3a855d46ab06e6ec",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:23:51.462768Z",
     "start_time": "2024-04-12T00:23:51.409051Z"
    }
   },
   "id": "4e60ad11cd96e14c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "EVAL_DATASET = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:23:58.383590Z",
     "start_time": "2024-04-12T00:23:51.463565Z"
    }
   },
   "id": "921990aa1cb355f0",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\"))\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:02.681752Z",
     "start_time": "2024-04-12T00:23:58.384446Z"
    }
   },
   "id": "11ea5ea55c0da346",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa1a8442cc4d4c0bbd8db020f5974fcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=\"gpt-4\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        disallowed_special=[],\n",
    "        allowed_special=\"all\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:02.737459Z",
     "start_time": "2024-04-12T00:24:02.682510Z"
    }
   },
   "id": "40aa9cb71de4a46a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model: Embeddings,\n",
    "    embedding_model_name: str\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model: the embedding\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "         \n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name}\"\n",
    "    index_folder_path = os.path.join(LANGCHAIN_DATA_ROOT, index_name)\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs\n",
    "        )\n",
    "        print(f\"Index not found, generating it... {len(docs_processed)} docs in total\")\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:02.793007Z",
     "start_time": "2024-04-12T00:24:02.739576Z"
    }
   },
   "id": "13dc9ec746ddadd2",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QA chain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74338ffcd9971f99"
  },
  {
   "cell_type": "code",
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:02.844392Z",
     "start_time": "2024-04-12T00:24:02.793638Z"
    }
   },
   "id": "851eb803b3f0a34f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:02.900349Z",
     "start_time": "2024-04-12T00:24:02.845105Z"
    }
   },
   "id": "283a72f828eb8ec3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating answers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeda09db98f634b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test function with langchain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110434a4f3c3b6d"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_langchain_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:02.954678Z",
     "start_time": "2024-04-12T00:24:02.900918Z"
    }
   },
   "id": "913cbbdae2722e2d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "def run_langchain_test_all() -> str:\n",
    "    \"\"\"\n",
    "    Build index and run langchain test with fixed parameter and model selections\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"langchain_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        embedding_model_name=EMBEDDING_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Running RAG with {settings_name}\")\n",
    "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "    run_langchain_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        llm=READER_LLM,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=True,\n",
    "        test_settings=settings_name,\n",
    "    )\n",
    "    \n",
    "    return output_file_name "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.008098Z",
     "start_time": "2024-04-12T00:24:02.955348Z"
    }
   },
   "id": "78de170f745561d",
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "# execute test for langchain\n",
    "LANGCHAIN_TEST_OUTPUT = run_langchain_test_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.426309Z",
     "start_time": "2024-04-12T00:24:03.008728Z"
    }
   },
   "id": "96d9c29a10db3e84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge base embeddings...\n",
      "Running RAG with langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71f3b6d2d87a4297ac323b80849bc9ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test function with doc-agent in instinct.cpp\n",
    "\n",
    "You have to manually start `doc-agent` locally.\n",
    "\n",
    "To build knowledge index with same knowledge base data from HF:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=./data/instinct/index.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  build \\\n",
    "  --force \\\n",
    "  --file=https://huggingface.co/api/datasets/m-ric/huggingface_doc/parquet/default/train/0.parquet \\\n",
    "  --type=PARQUET \\\n",
    "  --parquet_mapping=0:txt,1:metadata:source:varchar\n",
    "```\n",
    "\n",
    "To start http server for query:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=/tmp/rag_eval.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  serve \\\n",
    "  --port=9090 \n",
    "```\n",
    "\n",
    "Next, we will begin QA tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4580fc87b583de8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "394b91ee34890b32"
  },
  {
   "cell_type": "code",
   "source": [
    "def answer_with_doc_agent(question: str):\n",
    "    import requests\n",
    "    res = requests.post(\"http://localhost:9090/v1/chat/completions\", json={\"messages\": [{\"content\": question, \"role\": \"human\"}], \"stream\": False})\n",
    "    assert res.status_code == 200\n",
    "    body = res.json()\n",
    "    return body[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "\n",
    "def run_doc_agent_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = answer_with_doc_agent(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.481614Z",
     "start_time": "2024-04-12T00:24:03.427396Z"
    }
   },
   "id": "4afb09c4ab37a98f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "def run_doc_agent_test_all():\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"doc_agent_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "    print(f\"Running RAG with settings {settings_name}\")\n",
    "    run_doc_agent_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        output_file=output_file_name,\n",
    "        test_settings=settings_name\n",
    "    )\n",
    "    \n",
    "    return output_file_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.533109Z",
     "start_time": "2024-04-12T00:24:03.482451Z"
    }
   },
   "id": "4cceb41a4f1cbe75",
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "DOC_AGENT_TEST_OUTPUT = run_doc_agent_test_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.593865Z",
     "start_time": "2024-04-12T00:24:03.533940Z"
    }
   },
   "id": "66a1467f5bfb50f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with settings doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a361cdceb0c04bf588f7d9b03562a2da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Runner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdc11c2c8edc69d1"
  },
  {
   "cell_type": "code",
   "source": [
    "from ratelimit import limits,sleep_and_retry\n",
    "from retry import retry\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=6, period=60)\n",
    "def throttled_invoke(eval_chat_model, eval_prompt):\n",
    "    return eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "\n",
    "\n",
    "@retry(exceptions=Exception, tries=6)\n",
    "def evaluate_single_answer(\n",
    "        evaluation_prompt_template: ChatPromptTemplate,\n",
    "        experiment: dict,\n",
    "        throttled:bool,\n",
    "        eval_chat_model: BaseChatModel\n",
    "):\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "    if throttled:\n",
    "        eval_result = throttled_invoke(eval_chat_model, eval_prompt)\n",
    "    else:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    splits = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    assert len(splits) == 2\n",
    "    assert 1 <= int(splits[1]) <= 5\n",
    "    return splits\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    "    throttled:bool = True\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment and experiment[f\"eval_score_{evaluator_name}\"]:\n",
    "            continue\n",
    "        \n",
    "        splits = evaluate_single_answer(evaluation_prompt_template, experiment, throttled, eval_chat_model)\n",
    "        \n",
    "        if len(splits) != 2:\n",
    "            print(splits)\n",
    "            # experiment[f\"eval_score_{evaluator_name}\"] = \"\"\n",
    "            # experiment[f\"eval_feedback_{evaluator_name}\"] = \"\"\n",
    "            continue\n",
    "        feedback, score = splits \n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.651311Z",
     "start_time": "2024-04-12T00:24:03.594609Z"
    }
   },
   "id": "31856cff24422023",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.709348Z",
     "start_time": "2024-04-12T00:24:03.652048Z"
    }
   },
   "id": "c193a75198dabe56",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run evaluations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "292b22f2978e2d2c"
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_eval_results():\n",
    "    import glob\n",
    "    for output_file_name in glob.glob(\"./output/*.json\"):\n",
    "        print(f\"Evaluating {output_file_name}\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            EVAL_MODEL,\n",
    "            EVALUATOR_NAME,\n",
    "            EVALUATION_PROMPT_TEMPLATE,\n",
    "            # throttling is not needed for local model\n",
    "            False\n",
    "        )\n",
    "\n",
    "generate_eval_results()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T00:24:03.769392Z",
     "start_time": "2024-04-12T00:24:03.710023Z"
    }
   },
   "id": "83e219af8e9a45c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a53696779fb4ac7830747f1e8277b77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "912d019145814aeaa5bccfff72869991"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_eval_results():\n",
    "    import glob\n",
    "    outputs = []\n",
    "    for file in glob.glob(\"./output/*.json\"):\n",
    "        output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "        output[\"settings\"] = file\n",
    "        outputs.append(output)\n",
    "    return pd.concat(outputs)\n",
    "\n",
    "EVAL_RESULTS = load_eval_results()\n",
    "display(EVAL_RESULTS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T01:59:30.401112Z",
     "start_time": "2024-04-12T01:59:30.383764Z"
    }
   },
   "id": "b157cef7583830b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             question  \\\n",
       "0   What architecture is the `tokenizers-linux-x64...   \n",
       "1   What is the purpose of the BLIP-Diffusion mode...   \n",
       "2   How can a user claim authorship of a paper on ...   \n",
       "3   What is the purpose of the /healthcheck endpoi...   \n",
       "4   What is the default context window size for Lo...   \n",
       "..                                                ...   \n",
       "62  What is the maximum size of a model checkpoint...   \n",
       "63  What is the purpose of Weights and Biases (W&B...   \n",
       "64  What is the name of the open-source library cr...   \n",
       "65  What parameter is used to ensure that elements...   \n",
       "66  What is the command to install the latest vers...   \n",
       "\n",
       "                                          true_answer  \\\n",
       "0                           x86_64-unknown-linux-musl   \n",
       "1   The BLIP-Diffusion model is designed for contr...   \n",
       "2   By clicking their name on the corresponding Pa...   \n",
       "3                           Ensure the app is running   \n",
       "4                                          127 tokens   \n",
       "..                                                ...   \n",
       "62                                               10GB   \n",
       "63  To track their machine learning experiments at...   \n",
       "64                                            Optimum   \n",
       "65                                       equal_height   \n",
       "66  pip install --upgrade-strategy eager optimum[\"...   \n",
       "\n",
       "                                           source_doc  \\\n",
       "0   huggingface/tokenizers/blob/main/bindings/node...   \n",
       "1   huggingface/diffusers/blob/main/docs/source/en...   \n",
       "2   huggingface/hub-docs/blob/main/docs/hub/paper-...   \n",
       "3   huggingface/datasets-server/blob/main/services...   \n",
       "4   huggingface/transformers/blob/main/docs/source...   \n",
       "..                                                ...   \n",
       "62  huggingface/transformers/blob/main/docs/source...   \n",
       "63  gradio-app/gradio/blob/main/guides/06_integrat...   \n",
       "64                huggingface/blog/blob/main/intel.md   \n",
       "65  gradio-app/gradio/blob/main/guides/cn/03_build...   \n",
       "66  huggingface/diffusers/blob/main/docs/source/en...   \n",
       "\n",
       "                                     generated_answer  \\\n",
       "0   The `tokenizers-linux-x64-musl` binary is desi...   \n",
       "1   The context does not provide any information a...   \n",
       "2   The user can claim authorship of a paper on th...   \n",
       "3   The purpose of the `/healthcheck` endpoint in ...   \n",
       "4   The context window size for Local Attention in...   \n",
       "..                                                ...   \n",
       "62  The maximum size of a model checkpoint before ...   \n",
       "63  Sure, here are the specific roles and responsi...   \n",
       "64  The name of the open-source library created by...   \n",
       "65  Sure, here is the standalone question in its o...   \n",
       "66  The standalone question is about OpenVINO supp...   \n",
       "\n",
       "                                       retrieved_docs  \\\n",
       "0   [`tokenizers-linux-x64-musl`\\n\\nThis is the **...   \n",
       "1   [Stable Diffusion\\n\\n## Overview\\n\\nStable Dif...   \n",
       "2   [* Visit the Paper page.\\n* Filter for other m...   \n",
       "3   [Datasets server API - rows endpoint\\n\\n> /row...   \n",
       "4   [## Longformer Self Attention\\n\\nLongformer se...   \n",
       "..                                                ...   \n",
       "62                                                NaN   \n",
       "63                                                NaN   \n",
       "64                                                NaN   \n",
       "65                                                NaN   \n",
       "66                                                NaN   \n",
       "\n",
       "                                        test_settings  \\\n",
       "0   langchain_chunk:200_rerank:False_reader-model:...   \n",
       "1   langchain_chunk:200_rerank:False_reader-model:...   \n",
       "2   langchain_chunk:200_rerank:False_reader-model:...   \n",
       "3   langchain_chunk:200_rerank:False_reader-model:...   \n",
       "4   langchain_chunk:200_rerank:False_reader-model:...   \n",
       "..                                                ...   \n",
       "62  doc_agent_chunk:200_rerank:False_reader-model:...   \n",
       "63  doc_agent_chunk:200_rerank:False_reader-model:...   \n",
       "64  doc_agent_chunk:200_rerank:False_reader-model:...   \n",
       "65  doc_agent_chunk:200_rerank:False_reader-model:...   \n",
       "66  doc_agent_chunk:200_rerank:False_reader-model:...   \n",
       "\n",
       "   eval_score_mixtral:latest  \\\n",
       "0                          3   \n",
       "1                          3   \n",
       "2                          4   \n",
       "3                          5   \n",
       "4                          3   \n",
       "..                       ...   \n",
       "62                         3   \n",
       "63                         1   \n",
       "64                         3   \n",
       "65                         5   \n",
       "66                         2   \n",
       "\n",
       "                         eval_feedback_mixtral:latest  \\\n",
       "0   While the response correctly identifies that t...   \n",
       "1   The response correctly acknowledges that the c...   \n",
       "2   The response is largely correct and accurately...   \n",
       "3   The response accurately explains the purpose o...   \n",
       "4   The response correctly indicates that the cont...   \n",
       "..                                                ...   \n",
       "62  The response correctly states that the maximum...   \n",
       "63  The response does not correctly address the pu...   \n",
       "64  The response correctly states that the name of...   \n",
       "65  The response is correct and aligns with the re...   \n",
       "66  The response accurately acknowledges that the ...   \n",
       "\n",
       "                                             settings  \n",
       "0   ./output/rag_langchain_chunk:200_rerank:False_...  \n",
       "1   ./output/rag_langchain_chunk:200_rerank:False_...  \n",
       "2   ./output/rag_langchain_chunk:200_rerank:False_...  \n",
       "3   ./output/rag_langchain_chunk:200_rerank:False_...  \n",
       "4   ./output/rag_langchain_chunk:200_rerank:False_...  \n",
       "..                                                ...  \n",
       "62  ./output/rag_doc_agent_chunk:200_rerank:False_...  \n",
       "63  ./output/rag_doc_agent_chunk:200_rerank:False_...  \n",
       "64  ./output/rag_doc_agent_chunk:200_rerank:False_...  \n",
       "65  ./output/rag_doc_agent_chunk:200_rerank:False_...  \n",
       "66  ./output/rag_doc_agent_chunk:200_rerank:False_...  \n",
       "\n",
       "[134 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_mixtral:latest</th>\n",
       "      <th>eval_feedback_mixtral:latest</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What architecture is the `tokenizers-linux-x64...</td>\n",
       "      <td>x86_64-unknown-linux-musl</td>\n",
       "      <td>huggingface/tokenizers/blob/main/bindings/node...</td>\n",
       "      <td>The `tokenizers-linux-x64-musl` binary is desi...</td>\n",
       "      <td>[`tokenizers-linux-x64-musl`\\n\\nThis is the **...</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>3</td>\n",
       "      <td>While the response correctly identifies that t...</td>\n",
       "      <td>./output/rag_langchain_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the BLIP-Diffusion mode...</td>\n",
       "      <td>The BLIP-Diffusion model is designed for contr...</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en...</td>\n",
       "      <td>The context does not provide any information a...</td>\n",
       "      <td>[Stable Diffusion\\n\\n## Overview\\n\\nStable Dif...</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>3</td>\n",
       "      <td>The response correctly acknowledges that the c...</td>\n",
       "      <td>./output/rag_langchain_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can a user claim authorship of a paper on ...</td>\n",
       "      <td>By clicking their name on the corresponding Pa...</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/paper-...</td>\n",
       "      <td>The user can claim authorship of a paper on th...</td>\n",
       "      <td>[* Visit the Paper page.\\n* Filter for other m...</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>4</td>\n",
       "      <td>The response is largely correct and accurately...</td>\n",
       "      <td>./output/rag_langchain_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of the /healthcheck endpoi...</td>\n",
       "      <td>Ensure the app is running</td>\n",
       "      <td>huggingface/datasets-server/blob/main/services...</td>\n",
       "      <td>The purpose of the `/healthcheck` endpoint in ...</td>\n",
       "      <td>[Datasets server API - rows endpoint\\n\\n&gt; /row...</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>5</td>\n",
       "      <td>The response accurately explains the purpose o...</td>\n",
       "      <td>./output/rag_langchain_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the default context window size for Lo...</td>\n",
       "      <td>127 tokens</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "      <td>The context window size for Local Attention in...</td>\n",
       "      <td>[## Longformer Self Attention\\n\\nLongformer se...</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>3</td>\n",
       "      <td>The response correctly indicates that the cont...</td>\n",
       "      <td>./output/rag_langchain_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>What is the maximum size of a model checkpoint...</td>\n",
       "      <td>10GB</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "      <td>The maximum size of a model checkpoint before ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>3</td>\n",
       "      <td>The response correctly states that the maximum...</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the purpose of Weights and Biases (W&amp;B...</td>\n",
       "      <td>To track their machine learning experiments at...</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/06_integrat...</td>\n",
       "      <td>Sure, here are the specific roles and responsi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>1</td>\n",
       "      <td>The response does not correctly address the pu...</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What is the name of the open-source library cr...</td>\n",
       "      <td>Optimum</td>\n",
       "      <td>huggingface/blog/blob/main/intel.md</td>\n",
       "      <td>The name of the open-source library created by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>3</td>\n",
       "      <td>The response correctly states that the name of...</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What parameter is used to ensure that elements...</td>\n",
       "      <td>equal_height</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/03_build...</td>\n",
       "      <td>Sure, here is the standalone question in its o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>5</td>\n",
       "      <td>The response is correct and aligns with the re...</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What is the command to install the latest vers...</td>\n",
       "      <td>pip install --upgrade-strategy eager optimum[\"...</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en...</td>\n",
       "      <td>The standalone question is about OpenVINO supp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:...</td>\n",
       "      <td>2</td>\n",
       "      <td>The response accurately acknowledges that the ...</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T00:33:37.892782Z",
     "start_time": "2024-04-12T00:33:33.938502Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install duckdb",
   "id": "92176641e395e1d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\r\n",
      "  Downloading duckdb-0.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (763 bytes)\r\n",
      "Downloading duckdb-0.10.1-cp311-cp311-macosx_11_0_arm64.whl (14.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.3/14.3 MB\u001B[0m \u001B[31m28.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: duckdb\r\n",
      "Successfully installed duckdb-0.10.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T02:18:55.324140Z",
     "start_time": "2024-04-12T02:18:55.155910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get diffs\n",
    "import duckdb\n",
    "DIFF_SQL = \"SELECT tbl1.question, tbl1.true_answer, tbl1.generated_answer as langchain_answer, tbl1.score as langchain_score, tbl2.generated_answer as doc_agent_answer, tbl2.score as doc_agent_score \"\\\n",
    "           \"FROM \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'langchain%') AS tbl1 \"\\\n",
    "           \"JOIN \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'doc_agent%') AS tbl2 \"\\\n",
    "           \"ON tbl1.question = tbl2.question \" \\\n",
    "           f\"WHERE tbl1.score > tbl2.score\"\n",
    "\n",
    "DIFFS = duckdb.query(DIFF_SQL).to_df()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(DIFFS)\n",
    "\n",
    "DIFFS.to_excel(\"diffs.xlsx\")"
   ],
   "id": "609601e4b9513078",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                                                   question  \\\n",
       "0                                                               What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n   \n",
       "1                                                                     How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n",
       "2                                                            What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n   \n",
       "3                                                               What method is used to load a checkpoint for a task using `AutoPipeline`?\\n   \n",
       "4                                                                  What method does the EulerAncestralDiscreteScheduler use for sampling?\\n   \n",
       "5                                                                                         What is the purpose of the `gradio.Blocks` API?\\n   \n",
       "6                                           What command is used to install the requirements for a research project using 🤗 Transformers?\\n   \n",
       "7                                                                             What task does the `roberta-large-mnli` checkpoint perform?\\n   \n",
       "8                                                           What service is replacing the Paid tier of the Inference API at Hugging Face?\\n   \n",
       "9   What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\\n   \n",
       "10                                                     What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\\n   \n",
       "11                      What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\\n   \n",
       "12                                         What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\\n   \n",
       "13                                                             Where can you access the logs of your Endpoints in Hugging Face Endpoints?\\n   \n",
       "14                                                           What is the latest task added to Hugging Face AutoTrain for Computer Vision?\\n   \n",
       "15                                         What is the default repository type created by the `create_repo` function on Hugging Face Hub?\\n   \n",
       "16                                                     What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\\n   \n",
       "17                                      What file format is used to save and store PyTorch model weights more securely than `.bin` files?\\n   \n",
       "18                                                                            What type of security certification does Hugging Face have?\\n   \n",
       "19                                                What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\\n   \n",
       "20                                                                             What is the title of the paper introducing the ByT5 model?\\n   \n",
       "21                                                                   What is the dimension of the feature vector for the base BERT model?\\n   \n",
       "22                                                                                      What is the purpose of the 🧨 Diffusers tutorials?\\n   \n",
       "23                                                What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\\n   \n",
       "24                                       What transformation does the FNet model use to replace the self-attention layer in a BERT model?\\n   \n",
       "25                                          How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n   \n",
       "26                                                                                  What is the range of parameters for the LLaMA models?\\n   \n",
       "27                What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\\n   \n",
       "28                                                                  How can you install the Hugging Face Unity API in your Unity project?\\n   \n",
       "29                                                                     What is the pretraining objective of the Wav2Vec2 context network?\\n   \n",
       "30                                    What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\\n   \n",
       "31                What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\\n   \n",
       "32                                                        What command line module does PyTorch provide to run a script on multiple GPUs?\\n   \n",
       "33                                                            What is the command to upload an ESPnet model to a Hugging Face repository?\\n   \n",
       "34                                          How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n   \n",
       "35                                                    What is the command to install the latest version of Optimum with OpenVINO support?\\n   \n",
       "36                                                         What is the goal of the Named Entity Recognition task in token classification?\\n   \n",
       "\n",
       "                                                                                                                                                                                                      true_answer  \\\n",
       "0                                                                                                                                                                                       x86_64-unknown-linux-musl   \n",
       "1                                                By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n",
       "2                                                                                                                                                                                       Ensure the app is running   \n",
       "3                                                                                                                                                                                               from_pretrained()   \n",
       "4                                                                                                                                                                     Ancestral sampling with Euler method steps.   \n",
       "5                                          The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.   \n",
       "6                                                                                                                                                                                 pip install -r requirements.txt   \n",
       "7                                                                                                                                                                                             Text classification   \n",
       "8                                                                                                                                                                                             Inference Endpoints   \n",
       "9                                                                                                         Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.   \n",
       "10                                                                                         1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.   \n",
       "11                                                                                                                                                                                                          +800%   \n",
       "12                                                                                                                                                                                                           O(n)   \n",
       "13                                                                                                                                                             In the \"Logs\" tab of your Endpoint through the UI.   \n",
       "14                                                                                                                                                                                           Image Classification   \n",
       "15                                                                                                                                                                                                          model   \n",
       "16                    FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.   \n",
       "17                                                                                                                                                                                                 `.safetensors`   \n",
       "18                                                                                                                                                                                          SOC2 Type 2 certified   \n",
       "19                                                                                                                                                                                                 Beautiful Soup   \n",
       "20                                                                                                                                         ByT5: Towards a token-free future with pre-trained byte-to-byte models   \n",
       "21                                                                                                                                                                                                            768   \n",
       "22                                                                                                             To provide a gentle introduction to diffusion models and help understand the library fundamentals.   \n",
       "23                                                                                                                                                                                                       \"manual\"   \n",
       "24                                                                                                                                                                                              Fourier transform   \n",
       "25                                                                                                                                                                By passing `fp16=True` to the Accelerator init.   \n",
       "26                                                                                                                                                                                           7B to 65B parameters   \n",
       "27                                                                                                                                                                                                          HfApi   \n",
       "28  To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.   \n",
       "29     The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.   \n",
       "30                                                                                                                                                                 distilbert base uncased finetuned sst2 english   \n",
       "31                                                                                   To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.   \n",
       "32                                                                                                                                                                                                       torchrun   \n",
       "33                                                                                                                                       ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo   \n",
       "34                                                                                                                                                                                                     3-5 images   \n",
       "35                                                                                                                                                       pip install --upgrade-strategy eager optimum[\"openvino\"]   \n",
       "36                                                                           The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                langchain_answer  \\\n",
       "0                                                                                                 The `tokenizers-linux-x64-musl` binary is designed for the x86 architecture.\\n\\nThe context provides a list of binary names for different architectures, including `x86_64-unknown-linux-musl`, `aarch64-unknown-linux-musl`, `x86_64-unknown-linux-gnu`, `aarch64-unknown-linux-gnu`, `x86_64-pc-windows-msvc` and `aarch64-pc-windows-msvc`.   \n",
       "1                                                                                                                                                                                      The user can claim authorship of a paper on the Hugging Face Hub by clicking in their name in the corresponding Paper page and clicking \"claim authorship\". This will automatically re-direct to their paper settings where they can confirm the request.   \n",
       "2                                                                                                                                                                                                                  The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure the app is running properly. It sends a JSON response containing information about the cache, queue, and any pending jobs for the dataset.   \n",
       "3                                                                                                                                                                                                                                             The `from_pretrained()` method is used to load a checkpoint for a task using `AutoPipeline`. This method allows you to specify the path to the checkpoint file and the model architecture to load.   \n",
       "4                                                                                                                                                                                                                                                                                                                                              The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.   \n",
       "5                                                                                                                                                                                                                                   The purpose of the `gradio.Blocks` API is to allow developers to have full control over the data flows and layout of their application. It allows developers to build very complex, multi-step applications.   \n",
       "6                                                                                                                                                                                                                                                                                                       The command to install the requirements for a research project using 🤗 Transformers is:\\n\\n```bash\\npip install -r requirements.txt\\n```   \n",
       "7                                                                                                                                                                                                                                                                                The `roberta-large-mnli` checkpoint performs the task of fine-tuning a pre-trained RoBERTa model on the Multi Language Information in the Large (MNLI) dataset.   \n",
       "8                                                                                                                                                                                                                                                                    The context does not provide information about the service replacing the Paid tier of the Inference API at Hugging Face, so I cannot answer this question from the context.   \n",
       "9                                                                                                                                                                                            Sure, the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\\n\\n- Splitting the embedding matrix into two smaller matrices.\\n- Using repeating layers split among groups.   \n",
       "10                                                                                                                                                                                                                          Sure, here are the three main steps for fine-tuning a model with the 🤗 Datasets library:\\n\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.   \n",
       "11                                                                                                                                                                                                                                                      The passage states that Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.   \n",
       "12                                                                                           The time and memory complexity of the Nyströmformer's approximation of self-attention is O(n), where n is the length of the input sequence. The model uses a linear time and memory complexity by sampling landmarks (or Nyström points) from queries and keys, which can be used to construct matrices that approximate the self-attention matrix.   \n",
       "13                                                                                                                                                                                                                                                                                              According to the context, you can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the “Logs” tab of your Endpoint.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                  The latest task added to Hugging Face AutoTrain for Computer Vision is image classification.   \n",
       "15                                                                                                                                                                                                                                                                                                                                  The default repository type created by the `create_repo` function on Hugging Face Hub is the `dataset` type.   \n",
       "16  The purpose of Fully Sharded Data Parallel (FSDP) is to enable distributed training of large models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes. FSDP achieves memory efficiency by replicating the model across data parallel workers, reducing memory usage. Additionally, it allows for CPU offloading of all those tensors, enabling training of larger models.   \n",
       "17                                                                                                                                                                                                                        The file format used to save and store PyTorch model weights more securely than `.bin` files is the `safetensor` format. `safetensors` is a secure alternative to `pickle`, making it ideal for sharing model weights.   \n",
       "18                                                                                                                                                              According to the context, Hugging Face is [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning they provide security certification to their customers and actively monitor and patch any security weaknesses.   \n",
       "19                                                                                                                                                                                                                                                                                                                                     The library that MarkupLMFeatureExtractor uses to extract data from HTML and XML files is Beautiful Soup.   \n",
       "20                                                                                                                                                                                                                                                                              The title of the paper introducing the ByT5 model is [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626).   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                           The dimension of the feature vector for the base BERT model is 768.   \n",
       "22                                                                                                                                                                                                                                                                                                          The purpose of the 🧨 Diffusers tutorials is not explicitly stated in the context, so I cannot answer this question from the context.   \n",
       "23                                                                                                                                                                                                                                                                                                                                                  The default setting for the `allow_flagging` parameter in Gradio's `Interface` is **False**.   \n",
       "24                                                                                                                                                                                                                                                                                                                                                    The FNet model replaces the self-attention layer in a BERT model with a fourier transform.   \n",
       "25                                                                                                                                                                                                                                                                      The passage does not specify how to force mixed precision training when initializing the Accelerator in 🤗 Accelerate, so I cannot answer this question from the context.   \n",
       "26                                                                                                                                                                                                                                                      The range of parameters for the LLaMA models is from 7B to 65B.\\n\\nThe context states:\\n\\n> \"trainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\"   \n",
       "27                                                                                                                                                                                                                                                                                                             The `InferenceClient` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.   \n",
       "28                                                             Sure, here's how you can install the Hugging Face Unity API in your Unity project:\\n\\n1. Open your Unity project\\n2. Go to `Window` -> `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`   \n",
       "29                                                                                                                                                                                                                                                                                            The pretraining objective of the Wav2Vec2 context network is to find the most similar context vector and quantized speech unit (the target label).   \n",
       "30                                                                                                                                                                                                                                                                                                 The default checkpoint used by the sentiment analysis pipeline in the Transformers library is distilbert base uncased finetuned sst2 english.   \n",
       "31                                                                                                                                                                             The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to provide instructions on how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.   \n",
       "32                                                                                                                                                                                                                                                                   Sure, the `torchrun` module allows you to run a script on multiple GPUs.\\n\\nThe command would be:\\n\\n```bash\\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\\n```   \n",
       "33                                                                                                                                                                                                                                                                                           The context does not provide the command to upload an ESPnet model to a Hugging Face repository, so I cannot answer this question from the context.   \n",
       "34                                                                                                                                                                                                                                                        The context does not specify how many images are needed to teach new concepts to Stable Diffusion using Textual Inversion, so I cannot answer this question from the provided context.   \n",
       "35                                                                                                                                                                                                                                                                                      The command to install the latest version of Optimum with OpenVINO support is:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```   \n",
       "36                                                                                                                                                                                                                                                                            The goal of the Named Entity Recognition (NER) task in token classification is to find the entities in a piece of text, such as person, location, or organization.   \n",
       "\n",
       "   langchain_score  \\\n",
       "0                3   \n",
       "1                4   \n",
       "2                5   \n",
       "3                5   \n",
       "4                5   \n",
       "5                5   \n",
       "6                5   \n",
       "7                5   \n",
       "8                4   \n",
       "9                4   \n",
       "10               5   \n",
       "11               5   \n",
       "12               5   \n",
       "13               5   \n",
       "14               5   \n",
       "15               3   \n",
       "16               5   \n",
       "17               3   \n",
       "18               5   \n",
       "19               5   \n",
       "20               5   \n",
       "21               5   \n",
       "22               3   \n",
       "23               4   \n",
       "24               5   \n",
       "25               4   \n",
       "26               4   \n",
       "27               3   \n",
       "28               5   \n",
       "29               3   \n",
       "30               5   \n",
       "31               5   \n",
       "32               5   \n",
       "33               3   \n",
       "34               4   \n",
       "35               5   \n",
       "36               5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               doc_agent_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The `tokenizers-linux-arm64-musl` binary is a Linux binary for the `tokenizers` model that is designed to leverage CPU parallelism when possible. It is specifically designed for use on ARM processors running Linux.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The context does not provide any information about how a user can claim authorship of a paper on the Hugging Face Hub, so I cannot answer this question from the context.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The purpose of the /healthcheck endpoint in the Datasets server API is to ensure the app is running and to return a list of metrics in the Prometheus format. This endpoint is used to check the health of a dataset and to ensure that the app is running properly.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The method used to load a checkpoint for a task using `AutoPipeline` is by specifying the path to the checkpoint file as the argument to the `pipeline.load` method. This path can be obtained from the `pipeline.prompt` variable, which is an instance of the `AutoPrompt` class. The pipeline object has attributes that can be used to set parameters for the pipeline, including the path to the checkpoint file.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The sampling method used by the EulerAncestralDiscreteScheduler for generating samples is not specified in the question, so I cannot answer this question from the provided context.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The function of the `gradio.Blocks` API is not explicitly mentioned in the context. Therefore, I cannot answer this question from the context.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The specific command or procedure used in 🤗 Transformers for installing project requirements is not explicitly stated in the context. Therefore, I cannot provide an answer to this question from the context.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The purpose of the `roberta-large-mnli` checkpoint is to save the base model weights and configuration for the `MNLI` token classification model. This checkpoint allows you to initialize the model with the same weights and configuration as the original MNLI model.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The name of the service that is replacing the Paid tier of the Inference API at Hugging Face is not mentioned in the question, so I cannot answer this question from the provided context.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sure, here are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed:\\n\\n1. **Memory efficient attention**: This technique uses a fraction of the total memory to compute the attention weights, which helps to reduce the amount of memory required and improve the training speed.\\n\\n\\n2. **Trace UNet**: This technique uses a fraction of the total memory to compute the attention weights, which helps to reduce the amount of memory required and improve the training speed.   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Sure, here are the three main steps involved in fine-tuning a model with the 🤗 Datasets library:\\n\\n1. Setting up the data and the model parameters\\n2. Training the model on the data\\n3. Evaluating the model on the data   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is not explicitly mentioned in the context, so I cannot answer this question from the context.   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The time and memory complexity of the Nyströmformer's approximation of self-attention are not provided in the context, so I cannot answer this question from the provided context.   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The question is about where you can find the logs of your Endpoints in Hugging Face Endpoints. The context does not mention anything about logs, so I cannot answer this question from the chat history.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The context does not provide information about the latest task added to the Hugging Face AutoTrain for Computer Vision project, so I cannot answer this question from the provided context.   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The default repository type created by the `create_repo` function on Hugging Face Hub is a `GitHub` repository.   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to reduce the high cost of training large models. FSDP achieves this by dividing the model parameters, optimizer and gradient states across multiple GPUs or TPUs, allowing for efficient parallel training and reducing the communication overhead between the devices.   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The preferred file format for saving and storing PyTorch model weights to ensure more secure storage than `.bin` files is ` `.pt` files. These files use the PyTorch model format which is designed to be saveable and can be loaded without requiring any additional preprocessing.   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The context does not mention what kind of security certification Hugging Face holds, so I cannot answer this question from the provided context.   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The passage does not specify which library MarkupLMFeatureExtractor uses to extract data from HTML and XML files, so I cannot answer this question from the provided context.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The context does not provide the title of the paper that introduced the ByT5 model, so I cannot answer this question from the provided context.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The size of the feature vector for the base BERT model is not explicitly mentioned in the context, so I cannot answer this question from the provided context.   \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The function of the 🧨 Diffusers tutorials is not explicitly described in the question, so I cannot answer this question from the provided context.   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `True`. This means that the interface allows users to flag and interact with the model, which can be useful for debugging and understanding how the model works.   \n",
       "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The passage does not specify what the transformation is that the FNet model uses to replace the self-attention layer in a BERT model, so I cannot answer this question from the context.   \n",
       "25                                                                                              Sure, here is the standalone question in its original language:\\n\\n> How can I force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n\\nThe code you showed provides a good starting point for understanding mixed precision training in `torch` and `torch.accelerator`. Here are some additional details and steps you can take to force mixed precision training:\\n\\n- You need to set the `mixed_precision` parameter to `True` when initializing the `accelerator`.\\n- You need to use mixed precision training techniques like `torch.nn.mixed_precision_train()` and `torch.nn.mixed_precision_apply()` to define your mixed precision training strategy.\\n- You need to define the `device` on which you want to perform mixed precision training.\\n- You need to use the `torch.cuda.mixed_precision()` method to apply mixed precision training on your model.\\n\\nFor instance, you can set the `mixed_precision` parameter as follows:\\n\\n```python\\naccelerator = torch.accelerator.from_device(\"device\")\\naccelerator.mixed_precision = True\\n```\\n\\nThis will enable mixed precision training for your model on the specified device.   \n",
       "26  Sure, here are the answers to your rephrased question:\\n\\n**Parameters of the LLaMA models**\\n\\nThe LLaMA models have various parameters that can be adjusted to\\nchange their performance. These parameters include:\\n\\n- **Model size:** The size of the model, expressed in GB.\\n- **Number of parameters:** The total number of parameters in the model.\\n- **Data type:** The type of data used for training.\\n- **Quantization:** Whether the model is quantized or not.\\n- **Parameters of the model:** Specific parameters of the model, such as kernel size,\\nactivation functions, and pre-trained weights.\\n\\n**Values of these parameters**\\n\\nThe values of these parameters can be set by passing arguments to the\\nmodel constructor. For example, to set the model size to 10 GB, you can use the\\nconstructor method like this:\\n```\\nmodel = FastChat(model_size=10)\\n```\\n\\n**Impact of these parameters**\\n\\nThe values of these parameters can have a significant impact on the model's performance. For\\nexample, increasing the model size can increase the model's accuracy, but it can also\\nincrease the model's memory consumption. Setting the data type to \"c4\" can increase the model's\\nperformance, but it can also make it less efficient.   \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The original question was about the Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.   \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Sure, here is the standalone question with the answer:\\n\\n> How do I install the Hugging Face Unity API in my Unity project?\\n\\n> Follow the steps outlined in the blog post, starting with installation, and then proceed with setting up your API key, and optionally changing the model endpoints.   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The purpose of the pretraining objective of the Wav2Vec2 context network is to initialize the weights of the context network with a representation of the pre-trained language model. This helps to improve the quality of the representations and to make the context network more effective for speech recognition.   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The default checkpoint used by the sentiment analysis pipeline in the Transformers library is not specified in the context.   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Sure, here is the main purpose of the notebook you linked:\\n\\n> What is the main purpose of the notebook you linked?\\n\\n> This notebook demonstrates our first experience with Gaudi2, an efficient and high-performance AI hardware accelerator for the training and inference of machine learning models. We explore the benefits of Gaudi2 on performance and memory footprint over the first-generation Gaudi approach. Additionally, we fine-tune a T5-3B model with Gaudi2, showcasing its capabilities and potential for accelerated training on large datasets.   \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Sure, here is the standalone question in its original language:\\n\\n> What is the command line module provided by PyTorch that allows you to run a script on multiple GPUs?\\n\\nThe standalone question is about the command line module provided by PyTorch that allows you to run a script on multiple GPUs.   \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The command to upload an ESPnet model to a Hugging Face repository is:\\n\\n```\\nhub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\\n```   \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The text does not specify how many images are needed to teach new concepts to Stable Diffusion using Textual Inversion.   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The standalone question is about OpenVINO support for installation. It is not explicitly mentioned in the context, so I cannot answer this question from the provided context.   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The purpose of the Named Entity Recognition (NER) task within the context of token classification is to identify and extract named entities from the text. Named entities are important information that can be used for various tasks such as sentiment analysis, topic classification, and machine translation.   \n",
       "\n",
       "   doc_agent_score  \n",
       "0                1  \n",
       "1                3  \n",
       "2                4  \n",
       "3                1  \n",
       "4                2  \n",
       "5                2  \n",
       "6                3  \n",
       "7                3  \n",
       "8                3  \n",
       "9                1  \n",
       "10               3  \n",
       "11               3  \n",
       "12               3  \n",
       "13               1  \n",
       "14               4  \n",
       "15               1  \n",
       "16               4  \n",
       "17               2  \n",
       "18               4  \n",
       "19               4  \n",
       "20               4  \n",
       "21               3  \n",
       "22               2  \n",
       "23               1  \n",
       "24               3  \n",
       "25               3  \n",
       "26               3  \n",
       "27               1  \n",
       "28               3  \n",
       "29               2  \n",
       "30               3  \n",
       "31               3  \n",
       "32               1  \n",
       "33               1  \n",
       "34               3  \n",
       "35               2  \n",
       "36               4  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>langchain_answer</th>\n",
       "      <th>langchain_score</th>\n",
       "      <th>doc_agent_answer</th>\n",
       "      <th>doc_agent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n</td>\n",
       "      <td>x86_64-unknown-linux-musl</td>\n",
       "      <td>The `tokenizers-linux-x64-musl` binary is designed for the x86 architecture.\\n\\nThe context provides a list of binary names for different architectures, including `x86_64-unknown-linux-musl`, `aarch64-unknown-linux-musl`, `x86_64-unknown-linux-gnu`, `aarch64-unknown-linux-gnu`, `x86_64-pc-windows-msvc` and `aarch64-pc-windows-msvc`.</td>\n",
       "      <td>3</td>\n",
       "      <td>The `tokenizers-linux-arm64-musl` binary is a Linux binary for the `tokenizers` model that is designed to leverage CPU parallelism when possible. It is specifically designed for use on ARM processors running Linux.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n",
       "      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n",
       "      <td>The user can claim authorship of a paper on the Hugging Face Hub by clicking in their name in the corresponding Paper page and clicking \"claim authorship\". This will automatically re-direct to their paper settings where they can confirm the request.</td>\n",
       "      <td>4</td>\n",
       "      <td>The context does not provide any information about how a user can claim authorship of a paper on the Hugging Face Hub, so I cannot answer this question from the context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n</td>\n",
       "      <td>Ensure the app is running</td>\n",
       "      <td>The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure the app is running properly. It sends a JSON response containing information about the cache, queue, and any pending jobs for the dataset.</td>\n",
       "      <td>5</td>\n",
       "      <td>The purpose of the /healthcheck endpoint in the Datasets server API is to ensure the app is running and to return a list of metrics in the Prometheus format. This endpoint is used to check the health of a dataset and to ensure that the app is running properly.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What method is used to load a checkpoint for a task using `AutoPipeline`?\\n</td>\n",
       "      <td>from_pretrained()</td>\n",
       "      <td>The `from_pretrained()` method is used to load a checkpoint for a task using `AutoPipeline`. This method allows you to specify the path to the checkpoint file and the model architecture to load.</td>\n",
       "      <td>5</td>\n",
       "      <td>The method used to load a checkpoint for a task using `AutoPipeline` is by specifying the path to the checkpoint file as the argument to the `pipeline.load` method. This path can be obtained from the `pipeline.prompt` variable, which is an instance of the `AutoPrompt` class. The pipeline object has attributes that can be used to set parameters for the pipeline, including the path to the checkpoint file.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What method does the EulerAncestralDiscreteScheduler use for sampling?\\n</td>\n",
       "      <td>Ancestral sampling with Euler method steps.</td>\n",
       "      <td>The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.</td>\n",
       "      <td>5</td>\n",
       "      <td>The sampling method used by the EulerAncestralDiscreteScheduler for generating samples is not specified in the question, so I cannot answer this question from the provided context.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the purpose of the `gradio.Blocks` API?\\n</td>\n",
       "      <td>The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.</td>\n",
       "      <td>The purpose of the `gradio.Blocks` API is to allow developers to have full control over the data flows and layout of their application. It allows developers to build very complex, multi-step applications.</td>\n",
       "      <td>5</td>\n",
       "      <td>The function of the `gradio.Blocks` API is not explicitly mentioned in the context. Therefore, I cannot answer this question from the context.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What command is used to install the requirements for a research project using 🤗 Transformers?\\n</td>\n",
       "      <td>pip install -r requirements.txt</td>\n",
       "      <td>The command to install the requirements for a research project using 🤗 Transformers is:\\n\\n```bash\\npip install -r requirements.txt\\n```</td>\n",
       "      <td>5</td>\n",
       "      <td>The specific command or procedure used in 🤗 Transformers for installing project requirements is not explicitly stated in the context. Therefore, I cannot provide an answer to this question from the context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What task does the `roberta-large-mnli` checkpoint perform?\\n</td>\n",
       "      <td>Text classification</td>\n",
       "      <td>The `roberta-large-mnli` checkpoint performs the task of fine-tuning a pre-trained RoBERTa model on the Multi Language Information in the Large (MNLI) dataset.</td>\n",
       "      <td>5</td>\n",
       "      <td>The purpose of the `roberta-large-mnli` checkpoint is to save the base model weights and configuration for the `MNLI` token classification model. This checkpoint allows you to initialize the model with the same weights and configuration as the original MNLI model.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What service is replacing the Paid tier of the Inference API at Hugging Face?\\n</td>\n",
       "      <td>Inference Endpoints</td>\n",
       "      <td>The context does not provide information about the service replacing the Paid tier of the Inference API at Hugging Face, so I cannot answer this question from the context.</td>\n",
       "      <td>4</td>\n",
       "      <td>The name of the service that is replacing the Paid tier of the Inference API at Hugging Face is not mentioned in the question, so I cannot answer this question from the provided context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\\n</td>\n",
       "      <td>Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.</td>\n",
       "      <td>Sure, the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\\n\\n- Splitting the embedding matrix into two smaller matrices.\\n- Using repeating layers split among groups.</td>\n",
       "      <td>4</td>\n",
       "      <td>Sure, here are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed:\\n\\n1. **Memory efficient attention**: This technique uses a fraction of the total memory to compute the attention weights, which helps to reduce the amount of memory required and improve the training speed.\\n\\n\\n2. **Trace UNet**: This technique uses a fraction of the total memory to compute the attention weights, which helps to reduce the amount of memory required and improve the training speed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\\n</td>\n",
       "      <td>1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.</td>\n",
       "      <td>Sure, here are the three main steps for fine-tuning a model with the 🤗 Datasets library:\\n\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.</td>\n",
       "      <td>5</td>\n",
       "      <td>Sure, here are the three main steps involved in fine-tuning a model with the 🤗 Datasets library:\\n\\n1. Setting up the data and the model parameters\\n2. Training the model on the data\\n3. Evaluating the model on the data</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\\n</td>\n",
       "      <td>+800%</td>\n",
       "      <td>The passage states that Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.</td>\n",
       "      <td>5</td>\n",
       "      <td>The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is not explicitly mentioned in the context, so I cannot answer this question from the context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\\n</td>\n",
       "      <td>O(n)</td>\n",
       "      <td>The time and memory complexity of the Nyströmformer's approximation of self-attention is O(n), where n is the length of the input sequence. The model uses a linear time and memory complexity by sampling landmarks (or Nyström points) from queries and keys, which can be used to construct matrices that approximate the self-attention matrix.</td>\n",
       "      <td>5</td>\n",
       "      <td>The time and memory complexity of the Nyströmformer's approximation of self-attention are not provided in the context, so I cannot answer this question from the provided context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Where can you access the logs of your Endpoints in Hugging Face Endpoints?\\n</td>\n",
       "      <td>In the \"Logs\" tab of your Endpoint through the UI.</td>\n",
       "      <td>According to the context, you can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the “Logs” tab of your Endpoint.</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is about where you can find the logs of your Endpoints in Hugging Face Endpoints. The context does not mention anything about logs, so I cannot answer this question from the chat history.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the latest task added to Hugging Face AutoTrain for Computer Vision?\\n</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>The latest task added to Hugging Face AutoTrain for Computer Vision is image classification.</td>\n",
       "      <td>5</td>\n",
       "      <td>The context does not provide information about the latest task added to the Hugging Face AutoTrain for Computer Vision project, so I cannot answer this question from the provided context.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the default repository type created by the `create_repo` function on Hugging Face Hub?\\n</td>\n",
       "      <td>model</td>\n",
       "      <td>The default repository type created by the `create_repo` function on Hugging Face Hub is the `dataset` type.</td>\n",
       "      <td>3</td>\n",
       "      <td>The default repository type created by the `create_repo` function on Hugging Face Hub is a `GitHub` repository.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\\n</td>\n",
       "      <td>FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.</td>\n",
       "      <td>The purpose of Fully Sharded Data Parallel (FSDP) is to enable distributed training of large models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes. FSDP achieves memory efficiency by replicating the model across data parallel workers, reducing memory usage. Additionally, it allows for CPU offloading of all those tensors, enabling training of larger models.</td>\n",
       "      <td>5</td>\n",
       "      <td>The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to reduce the high cost of training large models. FSDP achieves this by dividing the model parameters, optimizer and gradient states across multiple GPUs or TPUs, allowing for efficient parallel training and reducing the communication overhead between the devices.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What file format is used to save and store PyTorch model weights more securely than `.bin` files?\\n</td>\n",
       "      <td>`.safetensors`</td>\n",
       "      <td>The file format used to save and store PyTorch model weights more securely than `.bin` files is the `safetensor` format. `safetensors` is a secure alternative to `pickle`, making it ideal for sharing model weights.</td>\n",
       "      <td>3</td>\n",
       "      <td>The preferred file format for saving and storing PyTorch model weights to ensure more secure storage than `.bin` files is ` `.pt` files. These files use the PyTorch model format which is designed to be saveable and can be loaded without requiring any additional preprocessing.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What type of security certification does Hugging Face have?\\n</td>\n",
       "      <td>SOC2 Type 2 certified</td>\n",
       "      <td>According to the context, Hugging Face is [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning they provide security certification to their customers and actively monitor and patch any security weaknesses.</td>\n",
       "      <td>5</td>\n",
       "      <td>The context does not mention what kind of security certification Hugging Face holds, so I cannot answer this question from the provided context.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\\n</td>\n",
       "      <td>Beautiful Soup</td>\n",
       "      <td>The library that MarkupLMFeatureExtractor uses to extract data from HTML and XML files is Beautiful Soup.</td>\n",
       "      <td>5</td>\n",
       "      <td>The passage does not specify which library MarkupLMFeatureExtractor uses to extract data from HTML and XML files, so I cannot answer this question from the provided context.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is the title of the paper introducing the ByT5 model?\\n</td>\n",
       "      <td>ByT5: Towards a token-free future with pre-trained byte-to-byte models</td>\n",
       "      <td>The title of the paper introducing the ByT5 model is [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626).</td>\n",
       "      <td>5</td>\n",
       "      <td>The context does not provide the title of the paper that introduced the ByT5 model, so I cannot answer this question from the provided context.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the dimension of the feature vector for the base BERT model?\\n</td>\n",
       "      <td>768</td>\n",
       "      <td>The dimension of the feature vector for the base BERT model is 768.</td>\n",
       "      <td>5</td>\n",
       "      <td>The size of the feature vector for the base BERT model is not explicitly mentioned in the context, so I cannot answer this question from the provided context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the purpose of the 🧨 Diffusers tutorials?\\n</td>\n",
       "      <td>To provide a gentle introduction to diffusion models and help understand the library fundamentals.</td>\n",
       "      <td>The purpose of the 🧨 Diffusers tutorials is not explicitly stated in the context, so I cannot answer this question from the context.</td>\n",
       "      <td>3</td>\n",
       "      <td>The function of the 🧨 Diffusers tutorials is not explicitly described in the question, so I cannot answer this question from the provided context.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\\n</td>\n",
       "      <td>\"manual\"</td>\n",
       "      <td>The default setting for the `allow_flagging` parameter in Gradio's `Interface` is **False**.</td>\n",
       "      <td>4</td>\n",
       "      <td>The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `True`. This means that the interface allows users to flag and interact with the model, which can be useful for debugging and understanding how the model works.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What transformation does the FNet model use to replace the self-attention layer in a BERT model?\\n</td>\n",
       "      <td>Fourier transform</td>\n",
       "      <td>The FNet model replaces the self-attention layer in a BERT model with a fourier transform.</td>\n",
       "      <td>5</td>\n",
       "      <td>The passage does not specify what the transformation is that the FNet model uses to replace the self-attention layer in a BERT model, so I cannot answer this question from the context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n</td>\n",
       "      <td>By passing `fp16=True` to the Accelerator init.</td>\n",
       "      <td>The passage does not specify how to force mixed precision training when initializing the Accelerator in 🤗 Accelerate, so I cannot answer this question from the context.</td>\n",
       "      <td>4</td>\n",
       "      <td>Sure, here is the standalone question in its original language:\\n\\n&gt; How can I force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n\\nThe code you showed provides a good starting point for understanding mixed precision training in `torch` and `torch.accelerator`. Here are some additional details and steps you can take to force mixed precision training:\\n\\n- You need to set the `mixed_precision` parameter to `True` when initializing the `accelerator`.\\n- You need to use mixed precision training techniques like `torch.nn.mixed_precision_train()` and `torch.nn.mixed_precision_apply()` to define your mixed precision training strategy.\\n- You need to define the `device` on which you want to perform mixed precision training.\\n- You need to use the `torch.cuda.mixed_precision()` method to apply mixed precision training on your model.\\n\\nFor instance, you can set the `mixed_precision` parameter as follows:\\n\\n```python\\naccelerator = torch.accelerator.from_device(\"device\")\\naccelerator.mixed_precision = True\\n```\\n\\nThis will enable mixed precision training for your model on the specified device.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the range of parameters for the LLaMA models?\\n</td>\n",
       "      <td>7B to 65B parameters</td>\n",
       "      <td>The range of parameters for the LLaMA models is from 7B to 65B.\\n\\nThe context states:\\n\\n&gt; \"trainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\"</td>\n",
       "      <td>4</td>\n",
       "      <td>Sure, here are the answers to your rephrased question:\\n\\n**Parameters of the LLaMA models**\\n\\nThe LLaMA models have various parameters that can be adjusted to\\nchange their performance. These parameters include:\\n\\n- **Model size:** The size of the model, expressed in GB.\\n- **Number of parameters:** The total number of parameters in the model.\\n- **Data type:** The type of data used for training.\\n- **Quantization:** Whether the model is quantized or not.\\n- **Parameters of the model:** Specific parameters of the model, such as kernel size,\\nactivation functions, and pre-trained weights.\\n\\n**Values of these parameters**\\n\\nThe values of these parameters can be set by passing arguments to the\\nmodel constructor. For example, to set the model size to 10 GB, you can use the\\nconstructor method like this:\\n```\\nmodel = FastChat(model_size=10)\\n```\\n\\n**Impact of these parameters**\\n\\nThe values of these parameters can have a significant impact on the model's performance. For\\nexample, increasing the model size can increase the model's accuracy, but it can also\\nincrease the model's memory consumption. Setting the data type to \"c4\" can increase the model's\\nperformance, but it can also make it less efficient.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\\n</td>\n",
       "      <td>HfApi</td>\n",
       "      <td>The `InferenceClient` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.</td>\n",
       "      <td>3</td>\n",
       "      <td>The original question was about the Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How can you install the Hugging Face Unity API in your Unity project?\\n</td>\n",
       "      <td>To install the Hugging Face Unity API in your Unity project, go to `Window` -&gt; `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.</td>\n",
       "      <td>Sure, here's how you can install the Hugging Face Unity API in your Unity project:\\n\\n1. Open your Unity project\\n2. Go to `Window` -&gt; `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -&gt; `Hugging Face API Wizard`</td>\n",
       "      <td>5</td>\n",
       "      <td>Sure, here is the standalone question with the answer:\\n\\n&gt; How do I install the Hugging Face Unity API in my Unity project?\\n\\n&gt; Follow the steps outlined in the blog post, starting with installation, and then proceed with setting up your API key, and optionally changing the model endpoints.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What is the pretraining objective of the Wav2Vec2 context network?\\n</td>\n",
       "      <td>The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.</td>\n",
       "      <td>The pretraining objective of the Wav2Vec2 context network is to find the most similar context vector and quantized speech unit (the target label).</td>\n",
       "      <td>3</td>\n",
       "      <td>The purpose of the pretraining objective of the Wav2Vec2 context network is to initialize the weights of the context network with a representation of the pre-trained language model. This helps to improve the quality of the representations and to make the context network more effective for speech recognition.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\\n</td>\n",
       "      <td>distilbert base uncased finetuned sst2 english</td>\n",
       "      <td>The default checkpoint used by the sentiment analysis pipeline in the Transformers library is distilbert base uncased finetuned sst2 english.</td>\n",
       "      <td>5</td>\n",
       "      <td>The default checkpoint used by the sentiment analysis pipeline in the Transformers library is not specified in the context.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\\n</td>\n",
       "      <td>To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.</td>\n",
       "      <td>The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to provide instructions on how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.</td>\n",
       "      <td>5</td>\n",
       "      <td>Sure, here is the main purpose of the notebook you linked:\\n\\n&gt; What is the main purpose of the notebook you linked?\\n\\n&gt; This notebook demonstrates our first experience with Gaudi2, an efficient and high-performance AI hardware accelerator for the training and inference of machine learning models. We explore the benefits of Gaudi2 on performance and memory footprint over the first-generation Gaudi approach. Additionally, we fine-tune a T5-3B model with Gaudi2, showcasing its capabilities and potential for accelerated training on large datasets.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What command line module does PyTorch provide to run a script on multiple GPUs?\\n</td>\n",
       "      <td>torchrun</td>\n",
       "      <td>Sure, the `torchrun` module allows you to run a script on multiple GPUs.\\n\\nThe command would be:\\n\\n```bash\\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\\n```</td>\n",
       "      <td>5</td>\n",
       "      <td>Sure, here is the standalone question in its original language:\\n\\n&gt; What is the command line module provided by PyTorch that allows you to run a script on multiple GPUs?\\n\\nThe standalone question is about the command line module provided by PyTorch that allows you to run a script on multiple GPUs.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What is the command to upload an ESPnet model to a Hugging Face repository?\\n</td>\n",
       "      <td>./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo</td>\n",
       "      <td>The context does not provide the command to upload an ESPnet model to a Hugging Face repository, so I cannot answer this question from the context.</td>\n",
       "      <td>3</td>\n",
       "      <td>The command to upload an ESPnet model to a Hugging Face repository is:\\n\\n```\\nhub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\\n```</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n</td>\n",
       "      <td>3-5 images</td>\n",
       "      <td>The context does not specify how many images are needed to teach new concepts to Stable Diffusion using Textual Inversion, so I cannot answer this question from the provided context.</td>\n",
       "      <td>4</td>\n",
       "      <td>The text does not specify how many images are needed to teach new concepts to Stable Diffusion using Textual Inversion.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What is the command to install the latest version of Optimum with OpenVINO support?\\n</td>\n",
       "      <td>pip install --upgrade-strategy eager optimum[\"openvino\"]</td>\n",
       "      <td>The command to install the latest version of Optimum with OpenVINO support is:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```</td>\n",
       "      <td>5</td>\n",
       "      <td>The standalone question is about OpenVINO support for installation. It is not explicitly mentioned in the context, so I cannot answer this question from the provided context.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>What is the goal of the Named Entity Recognition task in token classification?\\n</td>\n",
       "      <td>The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.</td>\n",
       "      <td>The goal of the Named Entity Recognition (NER) task in token classification is to find the entities in a piece of text, such as person, location, or organization.</td>\n",
       "      <td>5</td>\n",
       "      <td>The purpose of the Named Entity Recognition (NER) task within the context of token classification is to identify and extract named entities from the text. Named entities are important information that can be used for various tasks such as sentiment analysis, topic classification, and machine translation.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scoring evaluation results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfe8937f08d290a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "settings\n./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json    0.492537\n./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json    0.641791\nName: eval_score_mixtral:latest, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scoring_output(eval_result: pd.DataFrame, evaluator_name: str):\n",
    "    score_field = f\"eval_score_{evaluator_name}\"\n",
    "    result = eval_result.loc[:, [score_field, \"settings\"]].copy()\n",
    "    \n",
    "    result[score_field] = result[score_field].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "    \n",
    "    result[score_field] = (result[score_field] - 1) / 4    \n",
    "    average_scores = result.groupby(\"settings\")[score_field].mean()\n",
    "\n",
    "    average_scores.sort_values()\n",
    "    return average_scores\n",
    "\n",
    "scores = scoring_output(EVAL_RESULTS, EVALUATOR_NAME)\n",
    "display(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:39:19.130763Z",
     "start_time": "2024-04-03T06:39:19.095342Z"
    }
   },
   "id": "b9a50771a6f6daa9",
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
